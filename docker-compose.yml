services:
  # MinIO - S3-compatible object storage for our data lake
  # Web UI: http://localhost:9001 (login: minio/minio123)
  # S3 API: http://localhost:9000
  minio:
    image: minio/minio
    container_name: minio
    restart: unless-stopped # Automatically restart MinIO if it crashes
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}      # S3 access key from .env file
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}  # S3 secret key from .env file
    volumes:
      - ./minio-data:/data        # Persist data on host machine
    ports:
      - "9000:9000"               # S3 API endpoint
      - "9001:9001"               # Web console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Spark Master - distributed computing engine for ETL jobs
  # Master UI: http://localhost:8080
  # Spark jobs UI: http://localhost:4040 (when running)
  spark:
    image: apache/spark:3.5.0
    container_name: spark
    restart: unless-stopped # Automatically restart Spark if it crashes
    # environment:
    volumes:
      - ./jobs:/opt/spark/jobs    # Mount jobs directory
      - ./data:/opt/spark/data    # Mount data directory for CSV files
    ports:
      - "7077:7077"               # Spark master port (for workers to connect)
      - "8080:8080"               # Spark master web UI
      - "4040:4040"               # Spark application UI (appears when jobs run)
    command: >
      bash -c "
      wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -O /opt/spark/jars/hadoop-aws-3.3.4.jar &&
      wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1034/aws-java-sdk-bundle-1.11.1034.jar -O /opt/spark/jars/aws-java-sdk-bundle-1.11.1034.jar &&
      export AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER} &&
      export AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD} &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      "
    depends_on:
      - minio                     # Wait for MinIO to start first
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

# Spark Worker - executes the actual Spark tasks (scalable)
  spark-worker:
    image: apache/spark:3.5.0
    restart: unless-stopped # Automatically restart Spark worker if it crashes
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    volumes:
      - ./data:/opt/spark/data    # Add this line - worker needs access to data too
    command: >
      bash -c "
      wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -O /opt/spark/jars/hadoop-aws-3.3.4.jar &&
      wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.1034/aws-java-sdk-bundle-1.11.1034.jar -O /opt/spark/jars/aws-java-sdk-bundle-1.11.1034.jar &&
      export AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER} &&
      export AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD} &&
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark:7077
      "
    depends_on:
      spark:
        condition: service_healthy

   # Airflow Webserver - Web UI for managing workflows
  # Web UI: http://localhost:8082 (login: admin/admin)
  airflow-webserver:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-webserver
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: false
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: true
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 10
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      _AIRFLOW_DB_MIGRATE: true
      _AIRFLOW_WWW_USER_CREATE: true
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
    volumes: &airflow-common-volumes
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./airflow-data:/opt/airflow
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8082:8080"
    user: "50000:0"
    command: >
      bash -c "
      pip install --user apache-airflow-providers-docker requests &&
      airflow db migrate &&
      airflow users create --username ${AIRFLOW_ADMIN_USERNAME} --password ${AIRFLOW_ADMIN_PASSWORD} --firstname ${AIRFLOW_USER_FIRSTNAME} --lastname ${AIRFLOW_USER_LASTNAME} --role Admin --email ${AIRFLOW_USER_EMAIL} || true &&
      airflow webserver
      "
    depends_on:
      spark:
        condition: service_healthy

  # Airflow Scheduler - Executes tasks and manages workflow scheduling
  airflow-scheduler:
    image: apache/airflow:2.7.0-python3.9
    container_name: airflow-scheduler
    restart: unless-stopped
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__LOAD_EXAMPLES: false
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: true
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 10
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USERNAME}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
      _AIRFLOW_DB_MIGRATE: false
      _AIRFLOW_WWW_USER_CREATE: false
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./airflow-data:/opt/airflow
      - /var/run/docker.sock:/var/run/docker.sock
    user: "50000:0"
    command: >
      bash -c "
      pip install --user apache-airflow-providers-docker requests &&
      airflow scheduler
      "
    depends_on:
      - airflow-webserver
    healthcheck:
      test: ["CMD", "pgrep", "-f", "airflow scheduler"]
      interval: 30s
      timeout: 10s
      retries: 3

# HOW TO USE THIS STACK:
# 1. Run: docker-compose up
# 2. MinIO UI: http://localhost:9001 (create buckets, upload data)
# 3. Spark UI: http://localhost:8080 (monitor cluster)
# 4. Airflow UI: http://localhost:8082 (create/schedule ETL workflows)
# 5. Put your DAG files in ./dags/ folder - they'll sync automatically
# 6. In Spark jobs, access S3 data using: s3a://bucket-name/path/to/file